### 基础脉络

1. 基本概念
2. 求解一个策略的状态值以进行评价并进行改进，循环得到最优策略；分析状态值的工具：贝尔曼公式
3. 贝尔曼最优公式：对应最优策略。最优策略(沿着这个策略能得到最大状态值)
4. 开始学习求解最优策略的方法：值迭代，策略迭代和二者结合。它们都需要模型，需要迭代且每次迭代都需要policy update和value update
5. 蒙德卡罗方法：无模型求解，方法时在有数据(有随机变量的采样)时学习随机变量的期望值
6. 随机近似理论：为增量式算法(不需要全部数据采集完再更新)打基础
7. 随机()差分方法；Q-learning
8. 当状态值不再是表格形式(数据很多或连续)，需要用函数形式代替表格
9. 引入policy-based方法：直接改变策略而不是由策略得到值再改进策略
10. 结合policy-based和value-based方法





#### 基本概念

网格机器人例子:![image-20241018224012852](../../pictrue/image-20241018224012852.png)

state:location,二维位置

state space:state的集合(空间就是定义了规则的集合)

action:每个state下可采取的行动。在这里是向左向右等。注意action对state是依赖的，对不同的state，action也不同。action是state的函数

state transition:agent和环境的交互，从一个状态到另一个状态，如s1通过action2到s2：$s1\xrightarrow{a2}s2$

fobidden area:

情况1：可以进，但会被惩罚

情况2：不能进

后面均考虑情况1

state transition probalbility:用概率(具体来说是条件概率)考虑情况变化会更通用。比如从s1向右走(a2)，会因为有风等原因有0.1的概率直接到s3，则表示为:
$$
p(s2|s1,a2)=0.9\\
p(s3|s1,a2)=0.1\\
p(s_i|s1,a2)=0  \quad\forall i\ne2,3
$$
policy:策略帮我们选择一个action,表示为($\pi$默认表示策略)，如在s1时往上和往右五五开的策略
$$
\pi(a1|s1)=0.5\\
\pi(a2|s1)=0.5\\
\pi(a3|s1)=0\\
\pi(a4|s1)=0\\
\pi(a5|s1)=0\
$$
用表格表示为(表格可以表示概率情况)

![image-20241018230343108](../../pictrue/image-20241018230343108.png)

reward:采取action后的得到的数，一般正数代表鼓励，负数代表惩罚。一般化的表示还是用条件概率，如超出边界/进禁区-1:
$$
p(r=-1|s1,a1)\\
p(r\ne-1|s1,a1)=0
$$
一定要注意reward取决于当前state和action，而不是下一个state

trajectory：一个状态-动作-奖励链，它的return就是把沿着trajectory得到的reward。一个trajectory如下：$s1\xrightarrow[r=0]{a3}s4\xrightarrow[r=-1]{a3}s7\xrightarrow[r=0]{a2}s8\xrightarrow[r=+1]{a2}s9$

它的return就是0+(-1)+0+1=0

比较trjectory就是靠return

discounted return ：但如果是一个持续性的reward，它后面会一直选留在s9，这样return一直加显然不行。我们会在return前加一个$\gamma(\gamma<1)$.然后通过取不同的$\gamma$来调节系统:$\gamma$越小系统越看重开始的return，反之则越远视

episode：有限次任务(一般会有进步)



###### MDP中

![image-20241018232427214](../../pictrue/image-20241018232427214.png)

无记忆性:特定state采取特定action的概率和获得的reward和历史无关



### 贝尔曼公式



state value指的是在==固定策略下==一个state的价值，即state的数学期望。(和return的区别在于return是对于单个trajectory的，state value是针对多个trajectory的平均值。如果policy是确定的，那么state value和return是等价的)

![image-20241019113838900](../../pictrue/image-20241019113838900.png)

贝尔曼公式表示了state value之间的关系

贝尔曼公式的矩阵表示

![image-20241019112806299](../../pictrue/image-20241019112806299.png)

action value：

![image-20241019113739703](../../pictrue/image-20241019113739703.png)action value是在依赖于state，action和policy

显然有$v_\pi=\sum_a \pi(a|s)(q_\pi(s,a))$

![image-20241019114029665](../../pictrue/image-20241019114029665.png)

要注意的是即使策略不建议采取某个action，这个action value仍然可以计算以用来改进策略

如上图中$p(s1|a3)=0$但是$q_\pi(s1,a3)$也可以计算得到



### 贝尔曼最优公式

最优策略：在每个state的state value都优于其它policy

![image-20241019114701884](../../pictrue/image-20241019114701884.png)

这个max的意思是寻找能让v(s)值最大的policy